from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
import requests
import json
from google.cloud import bigquery
import logging
from airflow.models import Variable
import time

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables from the correct .env file
from dotenv import load_dotenv
load_dotenv('/opt/airflow/config/ll_hubspot.env')

# Ensure the HubSpot private token is correctly loaded
HUBSPOT_ACCESS_TOKEN = os.getenv('HUBSPOT_PRIVATE_TOKEN')
if not HUBSPOT_ACCESS_TOKEN:
    raise Exception("HubSpot Private Token is not found in environment variables.")

# BigQuery connection configuration
BIGQUERY_PROJECT_ID = 'rare-guide-433209-e6'
BIGQUERY_DATASET_ID = 'HUBSPOT'
TABLE_NAME = 'll_hs_contacts'

# Path to your service account JSON file
SERVICE_ACCOUNT_JSON = '/opt/airflow/config/rare-guide-433209-e6-7bc861ae2907.json'
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = SERVICE_ACCOUNT_JSON

# Default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# DAG definition
dag = DAG(
    'hubspot_to_bigquery_dag',
    default_args=default_args,
    description='Extract all HubSpot contact data incrementally and load into BigQuery',
    schedule_interval=timedelta(days=1),  # Runs daily
    catchup=False,
)

MAX_COLUMN_NAME_LENGTH = 300  # Adjusted to accommodate longer property names
STATE_VARIABLE_KEY = 'hubspot_last_modified_date'  # Airflow Variable key for state persistence

# Helper function to truncate column names
def truncate_column_name(column_name):
    return column_name[:MAX_COLUMN_NAME_LENGTH]

# Fetch a list of contact properties from HubSpot
def get_contact_properties():
    url = 'https://api.hubapi.com/properties/v1/contacts/properties'
    headers = {
        'Authorization': f'Bearer {HUBSPOT_ACCESS_TOKEN}',
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        logger.error(f"Failed to retrieve properties: {response.status_code} {response.reason}")
        return []

    properties = response.json()
    # Extract property names
    property_names = [prop['name'] for prop in properties]
    logger.info(f"Fetched {len(property_names)} properties.")
    return property_names

# Fetch contacts from HubSpot in batches and load into BigQuery
def fetch_and_load_hubspot_contacts(**context):
    client = bigquery.Client(project=BIGQUERY_PROJECT_ID)
    table_id = f"{BIGQUERY_PROJECT_ID}.{BIGQUERY_DATASET_ID}.{TABLE_NAME}"

    # Get contact properties
    properties = get_contact_properties()
    if not properties:
        logger.error("No contact properties found.")
        return

    # Ensure 'hs_lastmodifieddate' is included in properties
    if 'hs_lastmodifieddate' not in properties:
        properties.append('hs_lastmodifieddate')

    # Define the schema based on the properties
    schema = [
        bigquery.SchemaField('id', 'STRING', mode='REQUIRED'),
        bigquery.SchemaField('createdAt', 'TIMESTAMP'),
        bigquery.SchemaField('updatedAt', 'TIMESTAMP'),
        bigquery.SchemaField('hs_lastmodifieddate', 'TIMESTAMP'),
    ]
    for field in properties:
        if field in ['hs_lastmodifieddate']:  # Already added
            continue
        field_name = truncate_column_name(field)
        schema.append(bigquery.SchemaField(field_name, 'STRING'))

    # Create the table if it doesn't exist, or update the schema if it does
    try:
        table = client.get_table(table_id)
        logger.info(f"Table {table_id} already exists.")
        # Get existing schema fields
        existing_fields = {field.name for field in table.schema}
        new_fields = []
        for field in schema:
            if field.name not in existing_fields:
                new_fields.append(field)
        if new_fields:
            # Update schema to add new fields
            updated_schema = table.schema + new_fields
            table.schema = updated_schema
            table = client.update_table(table, ["schema"])
            logger.info(f"Updated table schema with {len(new_fields)} new fields.")
    except Exception as e:
        # Create the table
        table = bigquery.Table(table_id, schema=schema)
        table = client.create_table(table)
        logger.info(f"Created table {table_id}.")

    # Retrieve the last hs_lastmodifieddate timestamp from Airflow Variables
    last_modified_date = Variable.get(STATE_VARIABLE_KEY, default_var=None)
    if last_modified_date:
        try:
            # Parse last_modified_date from ISO format and convert to Unix timestamp in milliseconds
            last_modified_date_dt = datetime.strptime(last_modified_date, "%Y-%m-%dT%H:%M:%S.%fZ")
            last_modified_date_ts = int(last_modified_date_dt.timestamp() * 1000)
            logger.info(f"Resuming from last_modified_date: {last_modified_date_ts}")
        except ValueError as e:
            logger.error(f"Error parsing last_modified_date: {e}")
            last_modified_date_ts = None
    else:
        logger.info("No previous state found. Fetching all contacts.")
        last_modified_date_ts = None  # Initialize to None if no previous timestamp

    # Prepare to fetch data using the Search API
    url = 'https://api.hubapi.com/crm/v3/objects/contacts/search'
    headers = {
        'Authorization': f'Bearer {HUBSPOT_ACCESS_TOKEN}',
        'Content-Type': 'application/json',
    }

    total_records = 0
    max_modified_date_ts = last_modified_date_ts  # Initialize to last_modified_date_ts
    max_modified_date_iso = last_modified_date  # Initialize to last_modified_date

    # Implement date range pagination to handle 10,000 record limit
    start_date_ts = last_modified_date_ts or int(datetime(2020, 1, 1).timestamp() * 1000)
    end_date_ts = int(datetime.now().timestamp() * 1000)

    logger.info(f"Starting data extraction from {start_date_ts} to {end_date_ts}")

    while start_date_ts < end_date_ts:
        # Adjust date range to prevent exceeding 10,000 records
        # You may need to adjust the date range depending on your data volume
        next_date_ts = start_date_ts + (7 * 24 * 60 * 60 * 1000)  # Add 7 days in milliseconds
        if next_date_ts > end_date_ts:
            next_date_ts = end_date_ts

        logger.info(f"Fetching contacts updated between {start_date_ts} and {next_date_ts}")

        payload = {
            "filterGroups": [{
                "filters": [
                    {
                        "propertyName": "hs_lastmodifieddate",
                        "operator": "GTE",
                        "value": start_date_ts
                    },
                    {
                        "propertyName": "hs_lastmodifieddate",
                        "operator": "LT",
                        "value": next_date_ts
                    }
                ]
            }],
            "properties": properties,
            "limit": 100,
        }

        after = None
        has_more = True

        while has_more:
            if after:
                payload['after'] = after

            logger.debug(f"Payload being sent to HubSpot: {json.dumps(payload)}")

            try:
                response = requests.post(url, headers=headers, json=payload, timeout=30)
            except Exception as e:
                logger.error(f"Error fetching data from HubSpot: {e}")
                break

            if response.status_code != 200:
                logger.error(f"Error fetching data from HubSpot: {response.status_code} {response.text}")
                break

            data = response.json()
            results = data.get('results', [])
            logger.info(f"Number of contacts fetched in this request: {len(results)}")

            if not results:
                logger.info("No more contacts to fetch in this date range.")
                break

            # Transform data for BigQuery insertion
            rows_to_insert = []
            for contact in results:
                properties_data = contact.get('properties', {})
                updated_at_str = contact.get('updatedAt')
                created_at_str = contact.get('createdAt')
                hs_lastmodifieddate_str = properties_data.get('hs_lastmodifieddate')

                # Convert hs_lastmodifieddate to ISO format and timestamp in milliseconds
                if hs_lastmodifieddate_str:
                    hs_lastmodifieddate_ts = int(hs_lastmodifieddate_str)
                    hs_lastmodifieddate_dt = datetime.utcfromtimestamp(hs_lastmodifieddate_ts / 1000)
                    hs_lastmodifieddate_iso = hs_lastmodifieddate_dt.isoformat() + 'Z'

                    # Update max_modified_date_ts and max_modified_date_iso
                    if not max_modified_date_ts or hs_lastmodifieddate_ts > max_modified_date_ts:
                        max_modified_date_ts = hs_lastmodifieddate_ts
                        max_modified_date_iso = hs_lastmodifieddate_iso

                row = {
                    'id': contact['id'],
                    'createdAt': created_at_str,
                    'updatedAt': updated_at_str,
                    'hs_lastmodifieddate': hs_lastmodifieddate_iso if hs_lastmodifieddate_str else None,
                }

                # Add properties
                for field in properties:
                    field_name = truncate_column_name(field)
                    if field in ['hs_lastmodifieddate']:
                        continue  # Already added
                    value = properties_data.get(field)
                    row[field_name] = value if value is not None else None
                rows_to_insert.append(row)

            # Insert data into BigQuery
            try:
                errors = client.insert_rows_json(table_id, rows_to_insert)
                if errors:
                    logger.error(f"Errors while inserting rows: {errors}")
                else:
                    logger.info(f"Inserted {len(rows_to_insert)} rows into {table_id}.")
                    total_records += len(rows_to_insert)
            except Exception as e:
                logger.error(f"Exception while inserting rows: {e}")

            # Check for more pages
            after = data.get('paging', {}).get('next', {}).get('after')
            has_more = after is not None

            # Optional: Sleep to respect API rate limits
            # time.sleep(0.2)  # Sleep for 200 milliseconds

        # Move to the next date range
        start_date_ts = next_date_ts

    logger.info(f"Total records inserted: {total_records}")

    # Save the max_modified_date_iso back to Airflow Variables for the next run
    if max_modified_date_iso:
        Variable.set(STATE_VARIABLE_KEY, max_modified_date_iso)
        logger.info(f"Updated {STATE_VARIABLE_KEY} to {max_modified_date_iso}")

# Define task for the DAG
extract_and_load_task = PythonOperator(
    task_id='fetch_and_load_hubspot_contacts',
    python_callable=fetch_and_load_hubspot_contacts,
    dag=dag,
    provide_context=True,
)
